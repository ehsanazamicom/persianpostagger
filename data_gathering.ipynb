{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:purple\">What is CRISP DM Methodology?</h4>\n",
    "    \n",
    "    The CRoss Industry Standard Process for Data Mining (CRISP-DM) is a process model with six phases that naturally describes the data science life cycle.\n",
    "    \n",
    "    1- Business understanding – What does the business need?\n",
    "    2- Data understanding – What data do we have / need? Is it clean?\n",
    "    3- Data preparation – How do we organize the data for modeling?\n",
    "    4- Modeling – What modeling techniques should we apply?\n",
    "    5- Evaluation – Which model best meets the business objectives?\n",
    "    6- Deployment – How do stakeholders access the results?\n",
    "    \n",
    "<img src=\"assets/img/tutorial/crisp.jpg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<h4 style=\"color:darkred\">CRISP: 1-Business Understanding</h4>\n",
    "\n",
    "    In corpus linguistics, part-of-speech tagging, also called grammatical tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context.\n",
    "    \n",
    "    Finally, we want to label the word they give us as input in a sentence.\n",
    "    \n",
    "<img src=\"assets/img/tutorial/postagger.png\" width=\"300\" style=\"float:right;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libreries\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from PersianStemmer import PersianStemmer\n",
    "ps = PersianStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><h2 style=\"text-align:center; color: darkred\">CRISP: 2-Data Understanding Phase</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:purple\">What is our data?</h4>\n",
    "    \n",
    "    Our Data Source is the Dr. BijanKhan corpus that we want to make a dataset from it.\n",
    "\n",
    "<br><br>\n",
    "<i >BijanKhan Corpus<i>\n",
    "<img src=\"assets/img/output/maincorpus2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<b>Rewrite Main Corpus for making neccessary changes with RegularExpression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepair_txt_file():\n",
    "    with codecs.open('assets/maincorpus.txt', \"r\", \"utf-8\") as myfile:\n",
    "        data = myfile.read()\n",
    "\n",
    "        newData = re.sub('   +', '$', data)\n",
    "        newData = re.sub('ي', 'ی', newData)\n",
    "        newData = re.sub('ك', 'ک', newData)\n",
    "        #newData = re.sub('‌', ' ', data)\n",
    "\n",
    "    with codecs.open('output/improvedcorpus.txt', \"w\", \"utf-8\") as myfiles:\n",
    "        myfiles.write(newData)\n",
    "prepair_txt_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>improved corpus</i>\n",
    "<img src=\"assets/img/output/improvedcorpus.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<b>Convert corpus from .txt file to .CSV</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 406250: expected 2 fields, saw 3\\nSkipping line 406267: expected 2 fields, saw 3\\n'\n"
     ]
    }
   ],
   "source": [
    "def prepair_txt_file():\n",
    "    file = pd.read_csv(\"output/improvedcorpus.txt\", error_bad_lines=False, encoding='utf-8', delimiter = '$', quoting=csv.QUOTE_NONE)\n",
    "    file.to_csv('output/table.csv',encoding='utf-8-sig', index = None)\n",
    "prepair_txt_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>output</i>\n",
    "<img src=\"assets/img/output/table.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<b>Prepairing the DataFrame for processing & save it to new .csv file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.15774726867676\n"
     ]
    }
   ],
   "source": [
    "def column_array():\n",
    "    array = ['input', 'label', 'word len' , 'stem len', 'prefix len', 'suffix len'\n",
    "             , 'pre pos 1', 'pre pos 2', 'pre pos 3'\n",
    "             , 'nxt pos 1', 'nxt pos 2', 'nxt pos 3'\n",
    "            , 'suffix', 'suffix zamir', 'suffix exception', 'mokasar']\n",
    "    return array\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "def create_table(): \n",
    "    table = pd.read_csv(\"output/table.csv\", encoding='utf-8')\n",
    "    table.rename(columns={'#':'input',table.columns[1]:'label'}, inplace=True)\n",
    "    array = column_array()\n",
    "    for item in array:\n",
    "        if item != 'input' and item != 'label':\n",
    "            table[item] = np.nan\n",
    "    table.to_csv('output/datasetstructure.csv',encoding='utf-8-sig', index = None)\n",
    "\n",
    "\n",
    "create_table()\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print(elapsed)\n",
    "\n",
    "#tiktok 97.60322308540344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Dataset structure output</i>\n",
    "<img src=\"assets/img/output/datasetstructure.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<h4>Word_analysis() method is for get features from data</h4>\n",
    "<h4 style=\"color:purple\">Our suggested features</h4>\n",
    "    \n",
    "    word len: word length.\n",
    "    stem len: stem length.\n",
    "    preffix len: preffix length.\n",
    "    suffext len: suffext length.\n",
    "    pre pos 1: lable of first previous word.\n",
    "    pre pos 2: lable of second previous word.\n",
    "    pre pos 3: lable of third previous word.\n",
    "    next pos 1: lable of first next word.\n",
    "    next pos 2: lable of second next word.\n",
    "    next pos 3: lable of third next word.\n",
    "    suffix: such as \"سار\", \"سان\", \"لاخ\", \"مند\", \"دار\"\n",
    "    suffix zamir: such as \"م\", \"ت\", \"ش\"\n",
    "    suffix exception: such as \"تر\", \"ترین\", \"ام\", \"ات\"\n",
    "    mokasar: such as \"کتب\", \"کسورات\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analyse(word, operate):\n",
    "    if operate == 'word len':\n",
    "        return len(word)\n",
    "    if operate == 'stem len':\n",
    "        return len(ps.run(word))\n",
    "    if operate == 'prefix len':\n",
    "        stem = ps.run(word)\n",
    "        if len(stem) != len(word):\n",
    "            if word.endswith(stem) and len(stem) > 0:\n",
    "                split = word.split(stem)\n",
    "                return len(split[0])\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    if operate == 'suffix len':\n",
    "        stem = ps.run(word)\n",
    "        if len(stem) != len(word):\n",
    "            if word.startswith(stem) and len(stem) > 0:\n",
    "                split = word.split(stem)\n",
    "                return len(split[1])\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    if operate == 'suffix':\n",
    "        stem = ps.run(word)\n",
    "        suffix_arr = [\"كار\", \"ناك\", \"وار\", \"آسا\", \"آگین\", \"بار\", \"بان\", \"دان\", \"زار\", \"سار\", \"سان\", \"لاخ\", \"مند\", \"دار\", \"مرد\", \"کننده\", \"گرا\", \"نما\", \"متر\"]\n",
    "        if word_analyse(word, 'suffix len') > 0  and len(stem) > 0:\n",
    "            suffix = word.split(stem)[1]\n",
    "            suffix = re.sub('   +', '', suffix)\n",
    "            if suffix in suffix_arr:\n",
    "                return 1\n",
    "            else: \n",
    "                return 0\n",
    "        return 0\n",
    "    if operate == 'suffix zamir':\n",
    "        stem = ps.run(word)\n",
    "        suffixZamir = [\"م\", \"ت\", \"ش\"]\n",
    "        if word_analyse(word, 'suffix len') > 0  and len(stem) > 0:\n",
    "            suffix = word.split(stem)[1]\n",
    "            suffix = re.sub('   +', '', suffix)\n",
    "            if suffix in suffixZamir:\n",
    "                return 1\n",
    "            else: \n",
    "                return 0\n",
    "        return 0\n",
    "    if operate == 'suffix exception':\n",
    "        stem = ps.run(word)\n",
    "        suffixException = [\"ها\", \"تر\", \"ترین\", \"ام\", \"ات\", \"اش\"]\n",
    "        if word_analyse(word, 'suffix len') > 0  and len(stem) > 0:\n",
    "            suffix = word.split(stem)[1]\n",
    "            suffix = re.sub('   +', '', suffix)\n",
    "            if suffix in suffixException:\n",
    "                return 1\n",
    "            else: \n",
    "                return 0\n",
    "        return 0\n",
    "    if operate == 'mokasar':\n",
    "        stem = ps.run(word)\n",
    "        if len(stem) != len(word) and stem not in word:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "def column_array():\n",
    "    array = ['input', 'label', 'word len' , 'stem len', 'prefix len', 'suffix len'\n",
    "             , 'pre pos 1', 'pre pos 2', 'pre pos 3'\n",
    "             , 'nxt pos 1', 'nxt pos 2', 'nxt pos 3'\n",
    "            , 'suffix', 'suffix zamir', 'suffix exception', 'mokasar']\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "<br><br><br>\n",
    "<b>Go for make Features DataFrame chunk by chunk ;)</b>\n",
    "\n",
    "<b>Save each chunks to one csv for making final integrated Dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 200.0------ ChunkRemains: 199.0\n",
      "0.02992081642150879\n",
      "2 / 200.0------ ChunkRemains: 198.0\n",
      "0.048868417739868164\n",
      "3 / 200.0------ ChunkRemains: 197.0\n",
      "0.06781935691833496\n",
      "4 / 200.0------ ChunkRemains: 196.0\n",
      "0.09275078773498535\n",
      "5 / 200.0------ ChunkRemains: 195.0\n",
      "0.10970640182495117\n",
      "6 / 200.0------ ChunkRemains: 194.0\n",
      "0.12765836715698242\n",
      "7 / 200.0------ ChunkRemains: 193.0\n",
      "0.1486034393310547\n",
      "8 / 200.0------ ChunkRemains: 192.0\n",
      "0.1715536117553711\n",
      "9 / 200.0------ ChunkRemains: 191.0\n",
      "0.1874988079071045\n",
      "10 / 200.0------ ChunkRemains: 190.0\n",
      "0.20545029640197754\n",
      "11 / 200.0------ ChunkRemains: 189.0\n",
      "0.23337531089782715\n",
      "12 / 200.0------ ChunkRemains: 188.0\n",
      "83.44049620628357\n",
      "13 / 200.0------ ChunkRemains: 187.0\n",
      "171.97193932533264\n",
      "14 / 200.0------ ChunkRemains: 186.0\n",
      "254.09863805770874\n",
      "15 / 200.0------ ChunkRemains: 185.0\n",
      "335.3263463973999\n",
      "16 / 200.0------ ChunkRemains: 184.0\n",
      "421.12051916122437\n",
      "17 / 200.0------ ChunkRemains: 183.0\n",
      "523.7453479766846\n",
      "18 / 200.0------ ChunkRemains: 182.0\n",
      "628.0317368507385\n",
      "19 / 200.0------ ChunkRemains: 181.0\n",
      "724.5273406505585\n",
      "20 / 200.0------ ChunkRemains: 180.0\n",
      "822.7537302970886\n",
      "21 / 200.0------ ChunkRemains: 179.0\n",
      "914.9643957614899\n",
      "22 / 200.0------ ChunkRemains: 178.0\n",
      "1008.8922247886658\n",
      "23 / 200.0------ ChunkRemains: 177.0\n",
      "1091.424546957016\n",
      "24 / 200.0------ ChunkRemains: 176.0\n",
      "1177.187906742096\n",
      "25 / 200.0------ ChunkRemains: 175.0\n",
      "1271.7016696929932\n",
      "26 / 200.0------ ChunkRemains: 174.0\n",
      "1367.2605409622192\n",
      "27 / 200.0------ ChunkRemains: 173.0\n",
      "1466.7341523170471\n",
      "28 / 200.0------ ChunkRemains: 172.0\n",
      "1556.7279660701752\n",
      "29 / 200.0------ ChunkRemains: 171.0\n",
      "1646.5517072677612\n",
      "30 / 200.0------ ChunkRemains: 170.0\n",
      "1734.5770342350006\n",
      "31 / 200.0------ ChunkRemains: 169.0\n",
      "1842.524246454239\n",
      "32 / 200.0------ ChunkRemains: 168.0\n",
      "1942.4320766925812\n",
      "33 / 200.0------ ChunkRemains: 167.0\n",
      "2053.872517824173\n",
      "34 / 200.0------ ChunkRemains: 166.0\n",
      "2152.025260448456\n",
      "35 / 200.0------ ChunkRemains: 165.0\n",
      "2237.1170649528503\n",
      "36 / 200.0------ ChunkRemains: 164.0\n",
      "2324.4185593128204\n",
      "37 / 200.0------ ChunkRemains: 163.0\n",
      "2409.733404159546\n",
      "38 / 200.0------ ChunkRemains: 162.0\n",
      "2494.7490224838257\n",
      "39 / 200.0------ ChunkRemains: 161.0\n",
      "2580.1593136787415\n",
      "40 / 200.0------ ChunkRemains: 160.0\n",
      "2666.4344730377197\n",
      "41 / 200.0------ ChunkRemains: 159.0\n",
      "2751.029436349869\n",
      "42 / 200.0------ ChunkRemains: 158.0\n",
      "2837.866089105606\n",
      "43 / 200.0------ ChunkRemains: 157.0\n",
      "2924.4561021327972\n",
      "44 / 200.0------ ChunkRemains: 156.0\n",
      "3005.8666009902954\n",
      "45 / 200.0------ ChunkRemains: 155.0\n",
      "3096.225443840027\n",
      "46 / 200.0------ ChunkRemains: 154.0\n",
      "3197.4840252399445\n",
      "47 / 200.0------ ChunkRemains: 153.0\n",
      "3293.839210987091\n",
      "48 / 200.0------ ChunkRemains: 152.0\n",
      "3376.4449458122253\n",
      "49 / 200.0------ ChunkRemains: 151.0\n",
      "3453.161805152893\n",
      "50 / 200.0------ ChunkRemains: 150.0\n",
      "3530.858236551285\n",
      "51 / 200.0------ ChunkRemains: 149.0\n",
      "3620.2924025058746\n",
      "52 / 200.0------ ChunkRemains: 148.0\n",
      "3708.7020678520203\n",
      "53 / 200.0------ ChunkRemains: 147.0\n",
      "3802.0339484214783\n",
      "54 / 200.0------ ChunkRemains: 146.0\n",
      "3894.2162594795227\n",
      "55 / 200.0------ ChunkRemains: 145.0\n",
      "3974.741678714752\n",
      "56 / 200.0------ ChunkRemains: 144.0\n",
      "4053.564544916153\n",
      "57 / 200.0------ ChunkRemains: 143.0\n",
      "4136.536354780197\n",
      "58 / 200.0------ ChunkRemains: 142.0\n",
      "4221.188088178635\n",
      "59 / 200.0------ ChunkRemains: 141.0\n",
      "4302.239187002182\n",
      "60 / 200.0------ ChunkRemains: 140.0\n",
      "4389.176255226135\n",
      "61 / 200.0------ ChunkRemains: 139.0\n",
      "4474.509205341339\n",
      "62 / 200.0------ ChunkRemains: 138.0\n",
      "4556.713941812515\n",
      "63 / 200.0------ ChunkRemains: 137.0\n",
      "4634.363347768784\n",
      "64 / 200.0------ ChunkRemains: 136.0\n",
      "4719.158640384674\n",
      "65 / 200.0------ ChunkRemains: 135.0\n",
      "4805.359511852264\n",
      "66 / 200.0------ ChunkRemains: 134.0\n",
      "4895.702926635742\n",
      "67 / 200.0------ ChunkRemains: 133.0\n",
      "4977.4093153476715\n",
      "68 / 200.0------ ChunkRemains: 132.0\n",
      "5072.408087015152\n",
      "69 / 200.0------ ChunkRemains: 131.0\n",
      "5167.009702682495\n",
      "70 / 200.0------ ChunkRemains: 130.0\n",
      "5261.156366348267\n",
      "71 / 200.0------ ChunkRemains: 129.0\n",
      "5357.187682151794\n",
      "72 / 200.0------ ChunkRemains: 128.0\n",
      "5456.843642473221\n",
      "73 / 200.0------ ChunkRemains: 127.0\n",
      "5541.409810304642\n",
      "74 / 200.0------ ChunkRemains: 126.0\n",
      "5624.351144313812\n",
      "75 / 200.0------ ChunkRemains: 125.0\n",
      "5703.9725222587585\n",
      "76 / 200.0------ ChunkRemains: 124.0\n",
      "5782.005630016327\n",
      "77 / 200.0------ ChunkRemains: 123.0\n",
      "5859.849869251251\n",
      "78 / 200.0------ ChunkRemains: 122.0\n",
      "5942.537051677704\n",
      "79 / 200.0------ ChunkRemains: 121.0\n",
      "6029.866901636124\n",
      "80 / 200.0------ ChunkRemains: 120.0\n",
      "6121.6789445877075\n",
      "81 / 200.0------ ChunkRemains: 119.0\n",
      "6210.642122745514\n",
      "82 / 200.0------ ChunkRemains: 118.0\n",
      "6290.672867298126\n",
      "83 / 200.0------ ChunkRemains: 117.0\n",
      "6370.1895225048065\n",
      "84 / 200.0------ ChunkRemains: 116.0\n",
      "6455.720253705978\n",
      "85 / 200.0------ ChunkRemains: 115.0\n",
      "6546.105448246002\n",
      "86 / 200.0------ ChunkRemains: 114.0\n",
      "6633.855152130127\n",
      "87 / 200.0------ ChunkRemains: 113.0\n",
      "6717.8168523311615\n",
      "88 / 200.0------ ChunkRemains: 112.0\n",
      "6801.6517786979675\n",
      "89 / 200.0------ ChunkRemains: 111.0\n",
      "6888.254935741425\n",
      "90 / 200.0------ ChunkRemains: 110.0\n",
      "6978.091865539551\n",
      "91 / 200.0------ ChunkRemains: 109.0\n",
      "7060.897120475769\n",
      "92 / 200.0------ ChunkRemains: 108.0\n",
      "7140.763806819916\n",
      "93 / 200.0------ ChunkRemains: 107.0\n",
      "7225.432380437851\n",
      "94 / 200.0------ ChunkRemains: 106.0\n",
      "7310.350393533707\n",
      "95 / 200.0------ ChunkRemains: 105.0\n",
      "7395.701963663101\n",
      "96 / 200.0------ ChunkRemains: 104.0\n",
      "7474.929216861725\n",
      "97 / 200.0------ ChunkRemains: 103.0\n",
      "7567.770858764648\n",
      "98 / 200.0------ ChunkRemains: 102.0\n",
      "7658.133302927017\n",
      "99 / 200.0------ ChunkRemains: 101.0\n",
      "7742.6871790885925\n",
      "100 / 200.0------ ChunkRemains: 100.0\n",
      "7828.098600625992\n",
      "101 / 200.0------ ChunkRemains: 99.0\n",
      "7919.898069143295\n",
      "102 / 200.0------ ChunkRemains: 98.0\n",
      "7999.540510892868\n",
      "103 / 200.0------ ChunkRemains: 97.0\n",
      "8079.190656900406\n",
      "104 / 200.0------ ChunkRemains: 96.0\n",
      "8158.969999074936\n",
      "105 / 200.0------ ChunkRemains: 95.0\n",
      "8240.090386152267\n",
      "106 / 200.0------ ChunkRemains: 94.0\n",
      "8321.257770299911\n",
      "107 / 200.0------ ChunkRemains: 93.0\n",
      "8400.48600935936\n",
      "108 / 200.0------ ChunkRemains: 92.0\n",
      "8481.609592437744\n",
      "109 / 200.0------ ChunkRemains: 91.0\n",
      "8561.959589958191\n",
      "110 / 200.0------ ChunkRemains: 90.0\n",
      "8641.735202789307\n",
      "111 / 200.0------ ChunkRemains: 89.0\n",
      "8724.032908439636\n",
      "112 / 200.0------ ChunkRemains: 88.0\n",
      "8805.578701019287\n",
      "113 / 200.0------ ChunkRemains: 87.0\n",
      "8885.199877500534\n",
      "114 / 200.0------ ChunkRemains: 86.0\n",
      "8964.639906644821\n",
      "115 / 200.0------ ChunkRemains: 85.0\n",
      "9044.907396316528\n",
      "116 / 200.0------ ChunkRemains: 84.0\n",
      "9124.776066303253\n",
      "117 / 200.0------ ChunkRemains: 83.0\n",
      "9205.00370645523\n",
      "118 / 200.0------ ChunkRemains: 82.0\n",
      "9284.799870967865\n",
      "119 / 200.0------ ChunkRemains: 81.0\n",
      "9363.925713777542\n",
      "120 / 200.0------ ChunkRemains: 80.0\n",
      "9443.547275781631\n",
      "121 / 200.0------ ChunkRemains: 79.0\n",
      "9522.98643898964\n",
      "122 / 200.0------ ChunkRemains: 78.0\n",
      "9602.277892112732\n",
      "123 / 200.0------ ChunkRemains: 77.0\n",
      "9681.378674268723\n",
      "124 / 200.0------ ChunkRemains: 76.0\n",
      "9760.857121944427\n",
      "125 / 200.0------ ChunkRemains: 75.0\n",
      "9839.883800983429\n",
      "126 / 200.0------ ChunkRemains: 74.0\n",
      "9921.087559461594\n",
      "127 / 200.0------ ChunkRemains: 73.0\n",
      "10005.417365074158\n",
      "128 / 200.0------ ChunkRemains: 72.0\n",
      "10089.141756772995\n",
      "129 / 200.0------ ChunkRemains: 71.0\n",
      "10168.9808075428\n",
      "130 / 200.0------ ChunkRemains: 70.0\n",
      "10248.80129814148\n",
      "131 / 200.0------ ChunkRemains: 69.0\n",
      "10333.088375091553\n",
      "132 / 200.0------ ChunkRemains: 68.0\n",
      "10413.66839838028\n",
      "133 / 200.0------ ChunkRemains: 67.0\n",
      "10509.406020402908\n",
      "134 / 200.0------ ChunkRemains: 66.0\n",
      "10597.318841457367\n",
      "135 / 200.0------ ChunkRemains: 65.0\n",
      "10682.96850013733\n",
      "136 / 200.0------ ChunkRemains: 64.0\n",
      "10776.237996816635\n",
      "137 / 200.0------ ChunkRemains: 63.0\n",
      "10867.985491514206\n",
      "138 / 200.0------ ChunkRemains: 62.0\n",
      "10957.247690200806\n",
      "139 / 200.0------ ChunkRemains: 61.0\n",
      "11042.855636119843\n",
      "140 / 200.0------ ChunkRemains: 60.0\n",
      "11127.489033460617\n",
      "141 / 200.0------ ChunkRemains: 59.0\n",
      "11209.288917779922\n",
      "142 / 200.0------ ChunkRemains: 58.0\n",
      "11296.10639834404\n",
      "143 / 200.0------ ChunkRemains: 57.0\n",
      "11374.700281858444\n",
      "144 / 200.0------ ChunkRemains: 56.0\n",
      "11457.751075983047\n",
      "145 / 200.0------ ChunkRemains: 55.0\n",
      "11537.639700889587\n",
      "146 / 200.0------ ChunkRemains: 54.0\n",
      "11616.888839006424\n",
      "147 / 200.0------ ChunkRemains: 53.0\n",
      "11696.042309761047\n",
      "148 / 200.0------ ChunkRemains: 52.0\n",
      "11778.72346830368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 / 200.0------ ChunkRemains: 51.0\n",
      "11864.729574203491\n",
      "150 / 200.0------ ChunkRemains: 50.0\n",
      "11946.464850902557\n",
      "151 / 200.0------ ChunkRemains: 49.0\n",
      "12031.100084543228\n",
      "152 / 200.0------ ChunkRemains: 48.0\n",
      "12110.05702495575\n",
      "153 / 200.0------ ChunkRemains: 47.0\n",
      "12197.779732465744\n",
      "154 / 200.0------ ChunkRemains: 46.0\n",
      "12281.565051317215\n",
      "155 / 200.0------ ChunkRemains: 45.0\n",
      "12367.021774053574\n",
      "156 / 200.0------ ChunkRemains: 44.0\n",
      "12450.54115653038\n",
      "157 / 200.0------ ChunkRemains: 43.0\n",
      "12529.272244215012\n",
      "158 / 200.0------ ChunkRemains: 42.0\n",
      "12611.635533571243\n",
      "159 / 200.0------ ChunkRemains: 41.0\n",
      "12692.970117807388\n",
      "160 / 200.0------ ChunkRemains: 40.0\n",
      "12772.999831199646\n",
      "161 / 200.0------ ChunkRemains: 39.0\n",
      "12857.715972185135\n",
      "162 / 200.0------ ChunkRemains: 38.0\n",
      "12942.62639284134\n",
      "163 / 200.0------ ChunkRemains: 37.0\n",
      "13027.125674009323\n",
      "164 / 200.0------ ChunkRemains: 36.0\n",
      "13106.395667552948\n",
      "165 / 200.0------ ChunkRemains: 35.0\n",
      "13184.779160499573\n",
      "166 / 200.0------ ChunkRemains: 34.0\n",
      "13269.76384472847\n",
      "167 / 200.0------ ChunkRemains: 33.0\n",
      "13356.813663005829\n",
      "168 / 200.0------ ChunkRemains: 32.0\n",
      "13434.927967071533\n",
      "169 / 200.0------ ChunkRemains: 31.0\n",
      "13514.48282790184\n",
      "170 / 200.0------ ChunkRemains: 30.0\n",
      "13601.682878255844\n",
      "171 / 200.0------ ChunkRemains: 29.0\n",
      "13693.5459253788\n",
      "172 / 200.0------ ChunkRemains: 28.0\n",
      "13773.873251914978\n",
      "173 / 200.0------ ChunkRemains: 27.0\n",
      "13852.949219465256\n",
      "174 / 200.0------ ChunkRemains: 26.0\n",
      "13932.683447122574\n",
      "175 / 200.0------ ChunkRemains: 25.0\n",
      "14011.902216434479\n",
      "176 / 200.0------ ChunkRemains: 24.0\n",
      "14091.71434211731\n",
      "177 / 200.0------ ChunkRemains: 23.0\n",
      "14170.730165481567\n",
      "178 / 200.0------ ChunkRemains: 22.0\n",
      "14251.199880361557\n",
      "179 / 200.0------ ChunkRemains: 21.0\n",
      "14329.792531490326\n",
      "180 / 200.0------ ChunkRemains: 20.0\n",
      "14412.41324543953\n",
      "181 / 200.0------ ChunkRemains: 19.0\n",
      "14491.411180019379\n",
      "182 / 200.0------ ChunkRemains: 18.0\n",
      "14578.76328921318\n",
      "183 / 200.0------ ChunkRemains: 17.0\n",
      "14659.657560110092\n",
      "184 / 200.0------ ChunkRemains: 16.0\n",
      "14738.970689296722\n",
      "185 / 200.0------ ChunkRemains: 15.0\n",
      "14817.79329776764\n",
      "186 / 200.0------ ChunkRemains: 14.0\n",
      "14895.930454492569\n",
      "187 / 200.0------ ChunkRemains: 13.0\n",
      "14974.27049779892\n",
      "188 / 200.0------ ChunkRemains: 12.0\n",
      "15052.76608467102\n",
      "189 / 200.0------ ChunkRemains: 11.0\n",
      "15131.591027975082\n",
      "190 / 200.0------ ChunkRemains: 10.0\n",
      "15210.0718729496\n",
      "191 / 200.0------ ChunkRemains: 9.0\n",
      "15288.473832845688\n",
      "192 / 200.0------ ChunkRemains: 8.0\n",
      "15366.972126722336\n",
      "193 / 200.0------ ChunkRemains: 7.0\n",
      "15446.094501495361\n",
      "194 / 200.0------ ChunkRemains: 6.0\n",
      "15525.312117815018\n",
      "195 / 200.0------ ChunkRemains: 5.0\n",
      "15605.064125537872\n",
      "196 / 200.0------ ChunkRemains: 4.0\n",
      "15683.370277166367\n",
      "197 / 200.0------ ChunkRemains: 3.0\n",
      "15762.54213309288\n",
      "198 / 200.0------ ChunkRemains: 2.0\n",
      "15841.085747718811\n",
      "199 / 200.0------ ChunkRemains: 1.0\n",
      "15921.80237865448\n",
      "200 / 200.0------ ChunkRemains: 0.0\n",
      "16002.79188323021\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "header = True\n",
    "#row.name = 0\n",
    "ChunkSize = 10000\n",
    "chunkCount = 0\n",
    "for chunk in pd.read_csv(\"output/datasetstructure.csv\", encoding='utf-8', chunksize = ChunkSize, low_memory=False):\n",
    "###### for loop creating chunks - starts\n",
    "    if chunkCount > 10:\n",
    "    ###### for loop entire chunks starts\n",
    "        table = chunk\n",
    "        counter = 0\n",
    "        for index, row in table.iterrows():\n",
    "            #prepair vars\n",
    "            table_rows_num = len(table.index)\n",
    "            row_num = counter\n",
    "            col_num = len(table.columns)\n",
    "\n",
    "            #word_len\n",
    "            word = row['input']\n",
    "            table.iloc[[row_num],[2]] = word_analyse(word, 'word len')\n",
    "\n",
    "            #stem_len\n",
    "            table.iloc[[row_num],[3]] = word_analyse(word, 'stem len')\n",
    "\n",
    "            #prefix len\n",
    "            table.iloc[[row_num],[4]] = word_analyse(word, 'prefix len')\n",
    "\n",
    "            #suffix len\n",
    "            table.iloc[[row_num],[5]] = word_analyse(word, 'suffix len')                                        \n",
    "\n",
    "\n",
    "            #pre pos 1\n",
    "            if row_num-1 >= 0:\n",
    "                pos = table.iloc[[row_num-1],[1]].values[0][0]\n",
    "                table.iloc[[row_num],[6]] = pos\n",
    "                #break\n",
    "            else:\n",
    "                table.iloc[[row_num],[6]] = 0\n",
    "\n",
    "            #pre pos 2\n",
    "            if row_num-2 >= 0:\n",
    "                pos = table.iloc[[row_num-2],[1]].values[0][0]\n",
    "                table.iloc[[row_num],[7]] = pos\n",
    "                #break\n",
    "            else:\n",
    "                table.iloc[[row_num],[7]] = 0\n",
    "\n",
    "            #pre pos 3\n",
    "            if row_num-3 >= 0:\n",
    "                pos = table.iloc[[row_num-3],[1]].values[0][0]\n",
    "                table.iloc[[row_num],[8]] = pos\n",
    "                #break\n",
    "            else:\n",
    "                table.iloc[[row_num],[8]] = 0\n",
    "\n",
    "            #pre nxt 1\n",
    "            if row_num+1 < table_rows_num:\n",
    "                pos = table.iloc[[row_num+1],[1]].values[0][0]\n",
    "                table.iloc[[row_num],[9]] = pos\n",
    "                #break\n",
    "            else:\n",
    "                table.iloc[[row_num],[9]] = 0\n",
    "\n",
    "            #pre nxt 2\n",
    "            if row_num+2 < table_rows_num:\n",
    "                pos = table.iloc[[row_num+2],[1]].values[0][0]\n",
    "                table.iloc[[row_num],[10]] = pos\n",
    "                #break\n",
    "            else:\n",
    "                table.iloc[[row_num],[10]] = 0\n",
    "\n",
    "            #pre nxt 3\n",
    "            if row_num+3 < table_rows_num:\n",
    "                pos = table.iloc[[row_num+3],[1]].values[0][0]\n",
    "                table.iloc[[row_num],[11]] = pos\n",
    "                #break\n",
    "            else:\n",
    "                table.iloc[[row_num],[11]] = 0\n",
    "\n",
    "            #suffix\n",
    "            table.iloc[[row_num],[12]] = word_analyse(word, 'suffix')\n",
    "\n",
    "            #suffix\n",
    "            table.iloc[[row_num],[13]] = word_analyse(word, 'suffix zamir') \n",
    "\n",
    "            #suffix\n",
    "            table.iloc[[row_num],[14]] = word_analyse(word, 'suffix exception') \n",
    "\n",
    "            #suffix\n",
    "            table.iloc[[row_num],[15]] = word_analyse(word, 'mokasar') \n",
    "\n",
    "            counter = counter+1\n",
    "            if counter == ChunkSize:\n",
    "                counter = 1\n",
    "\n",
    "    \n",
    "    \n",
    "        ###### for loop entire chunks ends\n",
    "\n",
    "        array = column_array()\n",
    "        chunk.columns = array\n",
    "        chunk.to_csv('output/datatrain/datatrain'+str(chunkCount)+'.csv',encoding='utf-8-sig', header=header, mode='a')\n",
    "        gc.collect()\n",
    "        header = False\n",
    "\n",
    "#table.to_csv('table.csv',encoding='utf-8-sig', index = None)\n",
    "    chunkCount = chunkCount+1\n",
    "    chunkTotal = 2000000 / ChunkSize\n",
    "    remains = chunkTotal - chunkCount\n",
    "    print(str(chunkCount) + \" / \" + str(chunkTotal) + \"------ ChunkRemains: \" + str(remains))\n",
    "    elapsed = time.time() - t\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<b>combine chuncks into one CSV file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "path = \".\"\n",
    "os.chdir(path)\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for counter, current_file in enumerate(glob.glob(\"*.csv\")):\n",
    "    namedf = pd.read_csv(current_file, header=None, sep=\"|\")\n",
    "    results = pd.concat([results, namedf])\n",
    "\n",
    "#results.to_csv('output/datatrain/Combined.csv',encoding='utf-8-sig', index=None, header=None, sep=\"|\")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>output chunk files</i>\n",
    "<img src=\"assets/img/output/chunkfiles.jpg\">\n",
    "<br><br><br>\n",
    "<i>final version of DataSet</i>\n",
    "<img src=\"assets/img/output/dataset.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><h2 style=\"text-align:center; color: darkred\">End of Data Understanding Phase</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
